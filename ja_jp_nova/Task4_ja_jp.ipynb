{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# タスク 4: 会話型インターフェース - Nova Lite および Titan Embedding LLM とのチャット\n",
    "\n",
    "このノートブックでは、Amazon Bedrock の Nova Lite および Titan Embeddings Foundation Models (FM) を使用してチャットボットを構築します。\n",
    "\n",
    "チャットボットや仮想アシスタントなどの会話型インターフェースは、顧客のユーザーエクスペリエンスを向上させることができます。チャットボットは、自然言語処理 (NLP) と機械学習アルゴリズムを使用して、ユーザーのクエリを理解して応答します。チャットボットは、カスタマーサービス、販売、e コマースなどのさまざまなアプリケーションで使用して、ユーザーに迅速かつ効率的に応答できます。ユーザーは、Web サイト、ソーシャル メディア プラットフォーム、メッセージング アプリなどのさまざまなチャネルを通じてチャットボットにアクセスできます。\n",
    "\n",
    "- **チャットボット** (基本)、 FM モデルを使用したゼロショット チャットボット\n",
    "- **プロンプトを使用したチャットボット**、テンプレート (LangChain) - プロンプトテンプレートで提供されるコンテキストを持つチャットボット\n",
    "- **ペルソナを使用したチャットボット**、定義されたロールを持つチャットボット。つまり、キャリアコー​​チと人間のやり取り\n",
    "- **コンテキスト認識型チャットボット**、埋め込みを生成することで外部ファイルを介してコンテキストを渡します。\n",
    "\n",
    "## Amazon Bedrock でチャットボットを構築するための LangChain フレームワーク\n",
    "\n",
    "チャットボットなどの会話型インターフェイスでは、短期的にも長期的にも、以前のやり取りを記憶することが非常に重要になります。\n",
    "\n",
    "LangChain フレームワークは、2 つの形式でメモリ コンポーネントを提供します。まず、LangChain は以前のチャット メッセージを管理および操作するためのヘルパーユーティリティを提供します。これらはモジュールとして機能するように設計されています。次に、LangChain はこれらのユーティリティをチェーンに組み込む簡単な方法を提供し、さまざまなタイプの抽象化を簡単に定義して操作できるようにすることで、強力なチャットボットを簡単に構築できるようにします。\n",
    "\n",
    "## コンテキストを使用したチャットボットの構築 - 主要な要素\n",
    "\n",
    "コンテキスト認識型チャットボットを構築する最初のプロセスは、コンテキストの埋め込みを生成することです。通常、取り込みプロセスは埋め込みモデルを実行し、埋め込みを生成して、ベクターストアに保存します。このノートブックでは、Titan Embeddings モデルを使用します。2 番目のプロセスは、ユーザー リクエストのオーケストレーション、インタラクション、呼び出し、および結果の返却です。これには、ユーザー リクエストのオーケストレーション、情報収集に必要なモデル/コンポーネントとのインタラクション、応答を作成するためのチャットボットの呼び出し、およびチャットボットの応答をユーザーに返すことが含まれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1: 環境のセットアップ\n",
    "\n",
    "このタスクでは、環境をセットアップします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ignore warnings and create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime',region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### リソース最適化ユーティリティ\n",
    "\n",
    "このセクションでは、API リソースの使用を最適化し、リクエストの信頼性を向上させるデコレータを作成します。このデコレータは、クラウドベースの AI サービスを使用するためのベストプラクティスをいくつか実装しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from functools import wraps\n",
    "\n",
    "# Resource optimization decorator for API efficiency\n",
    "def optimize_resource_usage(max_attempts=10, base_pause=10.0):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    # Space out requests for optimal service performance\n",
    "                    if attempt > 0:\n",
    "                        time.sleep(10.0)\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    error_str = str(e)\n",
    "                    # Check if this is a service capacity exception\n",
    "                    if any(err in error_str for err in [\"ThrottlingException\", \"TooManyRequests\", \n",
    "                                                     \"Too many tokens\", \"Rate exceeded\"]):\n",
    "                        if attempt < max_attempts - 1:\n",
    "                            # Implement progressive backoff with jitter\n",
    "                            jitter = random.random() * 0.5\n",
    "                            wait_time = min(base_pause * (2 ** attempt) + jitter, 60)\n",
    "                            \n",
    "                            print(f\"⏱️ Optimizing request timing. Pausing for {wait_time:.1f}s (attempt {attempt+1}/{max_attempts})\")\n",
    "                            time.sleep(wait_time)\n",
    "                        else:\n",
    "                            print(\"⚠️ Maximum attempts reached. Consider spacing out operations.\")\n",
    "                            raise\n",
    "                    else:\n",
    "                        raise\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### リソース最適化について\n",
    "\n",
    "上記のコードは、API 管理に対するインテリジェントなアプローチを実装しています。具体的には、次のようになります。\n",
    "\n",
    "- **リクエストを分散** して最適なスループットを維持します。\n",
    "- **一時的な容量制限に達した場合に自動的に再試行**します。\n",
    "- **待機時間を徐々に増やすことで指数バックオフを実装**します。\n",
    "- **ランダムジッターを追加**してリクエストのクラスタリングを防止します。\n",
    "\n",
    "これらの手法は、クラウドベースの AI サービスを使用する際の業界標準の手法であり、困難な状況下でもアプリケーションの応答性を維持します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 強化されたBedrockモデルクラス\n",
    "\n",
    "次に、リソース最適化技術を自動的に組み込んだBedrockモデルクラスの特殊バージョンを作成します。これらのクラスは、標準のLangChain実装を拡張するとともに、本番環境アプリケーションに不可欠な信頼性機能を追加します。\n",
    "\n",
    "重要なAPIメソッドを最適化デコレータでラップすることで、Bedrockサービスとの通信時に一貫した動作を保証します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "\n",
    "# Enhanced ChatBedrock with resource optimization\n",
    "class ResourceOptimizedChatBedrock(ChatBedrock):\n",
    "    \"\"\"Wrapper that optimizes resource usage and handles service capacity constraints.\"\"\"\n",
    "    \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def _call_model(self, *args, **kwargs):\n",
    "        return super()._call_model(*args, **kwargs)\n",
    "    \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def _generate(self, *args, **kwargs):\n",
    "        return super()._generate(*args, **kwargs)\n",
    "        \n",
    "# Enhanced BedrockEmbeddings with resource optimization\n",
    "class ResourceOptimizedEmbeddings(BedrockEmbeddings):\n",
    "    \"\"\"Wrapper for BedrockEmbeddings with intelligent resource management.\"\"\"\n",
    "    \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def _embed_documents(self, texts):\n",
    "        return super()._embed_documents(texts)\n",
    "        \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def _embed_query(self, text):\n",
    "        return super()._embed_query(text)\n",
    "        \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def embed_documents(self, texts):\n",
    "        return super().embed_documents(texts)\n",
    "        \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def embed_query(self, text):\n",
    "        return super().embed_query(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **注:** これらの拡張クラスは以下の機能を提供します。\n",
    "\n",
    "- すべての重要なAPIメソッドにリソース最適化を適用します。\n",
    "- 標準のLangChainインターフェースとの完全な互換性を維持します。\n",
    "- 断続的なサービスキャパシティの問題を自動的に処理します。\n",
    "- チャットと埋め込み操作の両方で一貫した動作を提供します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 会話メモリの構築\n",
    "\n",
    "効果的なチャットボットにとって最も重要な機能の一つは、過去のやり取りを記憶する能力です。記憶がなければ、各メッセージは個別に扱われ、一貫性のない体験を生み出してしまいます。\n",
    "\n",
    "このセクションでは、LangChainの`InMemoryChatMessageHistory`クラスを用いて会話メモリを実装します。これにより、チャットボットは過去のやり取りを参照し、会話全体を通してコンテキストを維持できるようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 会話フォーマットユーティリティ\n",
    "\n",
    "このセクションでは、大規模言語モデルにおける複数役割の会話を正しくフォーマットするユーティリティ関数を実装します。フォーマットは、Llama 3 モデルトークンフォーマットの特定の要件に準拠しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format instructions into a conversational prompt\n",
    "from typing import Dict, List\n",
    "\n",
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format instructions where conversation roles must alternate system/user/assistant/user/assistant/...\"\"\"\n",
    "    prompt: List[str] = []\n",
    "    for instruction in instructions:\n",
    "        if instruction[\"role\"] == \"system\":\n",
    "            prompt.extend([\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        elif instruction[\"role\"] == \"user\":\n",
    "            prompt.extend([\"<|start_header_id|>user<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid role: {instruction['role']}. Role must be either 'user' or 'system'.\")\n",
    "    prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## タスク 4.2: LangChain の chat history を使用して会話を開始する\n",
    "\n",
    "このタスクでは、チャットボットがユーザーとの複数のやり取りにわたって会話のコンテキストを保持できるようにします。チャットボットが長期にわたって意味のある一貫した対話を行うには、会話メモリが不可欠です。\n",
    "\n",
    "会話メモリ機能は、LangChain の InMemoryChatMessageHistory クラス上に構築して実装します。このオブジェクトには、ユーザーとチャットボット間の会話が保存され、チャットボットエージェントは履歴を利用できるため、以前の会話のコンテキストを活用できます。\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** モデルの出力は非決定的です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "chat_model = ResourceOptimizedChatBedrock(\n",
    "    model_id=\"amazon.nova-lite-v1:0\", \n",
    "    client=bedrock_client\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"以下の質問にできるだけ正確に答えてください。\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "query=\"お元気ですか？\"\n",
    "response=wrapped_chain.invoke({\"input\": query})\n",
    "# Printing history to see the history being built out. \n",
    "print(history)\n",
    "# For the rest of the conversation, the output will only include response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新しい質問\n",
    "\n",
    "モデルは最初のメッセージで応答しました。次に、モデルにいくつか質問します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#new questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"新しくガーデニングを始めるためのヒントをいくつか教えてください。\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 質問を基に構築\n",
    "\n",
    "次に、garden という言葉を使わずに質問をして、モデルが前の会話を理解できるかどうかを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build on the questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"虫についてはどうですか？\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### この会話を終える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# finishing the conversation\n",
    "instructions = [{\"role\": \"user\", \"content\": \"以上です、ありがとうございました！\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## タスク 4.3: プロンプトテンプレートを使用したチャットボット (LangChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一貫した対話のためのプロンプトテンプレートの使用\n",
    "\n",
    "プロンプトエンジニアリングは、大規模言語モデルを扱う上で重要な側面です。テンプレートを使用することで、プロンプトを一貫した方法で構造化し、異なるシナリオでモデルがどのように応答するかを制御できます。\n",
    "\n",
    "LangChain のプロンプトテンプレートは、異なる会話の役割（システム、ユーザー、アシスタント）に対する入力を標準化された方法でフォーマットし、モデルが適切に構造化されたコンテキストを受け取ることを保証します。\n",
    "\n",
    "このタスクでは、この入力の構築を担当するデフォルトの PromptTemplate を使用します。LangChain は、プロンプトの構築と操作を容易にするためのクラスと関数をいくつか提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  prompt for a conversational agent\n",
    "def format_prompt(actor:str, input:str):\n",
    "    formatted_prompt: List[str] = []\n",
    "    if actor == \"system\":\n",
    "        prompt_template=\"\"\"<|begin_of_text|><|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    elif actor == \"user\":\n",
    "        prompt_template=\"\"\"<|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid role: {actor}. Role must be either 'user' or 'system'.\")   \n",
    "    prompt = PromptTemplate.from_template(prompt_template)     \n",
    "    formatted_prompt.extend(prompt.format(actor=actor,input=input))\n",
    "    formatted_prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chat user experience\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa, retrievalChain = False):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.retrievalChain = retrievalChain\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Starting chat bot\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else: \n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            with self.out:\n",
    "                print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.retrievalChain:\n",
    "                        response = self.qa.invoke({\"input\": prompt})\n",
    "                        result=response['answer']\n",
    "                    else:\n",
    "                        instructions = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                        #result = self.qa.invoke({'input': format_prompt(\"user\",prompt)}) #, 'history':chat_history})\n",
    "                        result = self.qa.invoke({\"input\": format_instructions(instructions)})\n",
    "                except:\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print(f\"AI:{result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You:\", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、チャットを開始します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start chat\n",
    "history = InMemoryChatMessageHistory() #reset chat history\n",
    "chat = ChatUX(wrapped_chain)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## タスク 4.4: ペルソナ付きチャットボット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このタスクでは、人工知能 (AI) アシスタントがキャリアコー​​チの役割を果たします。システムメッセージを使用して、チャットボットにペルソナ (または役割) を通知できます。会話のコンテキストを維持するために、InMemoryChatMessageHistory クラスを引き続き活用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたはキャリアコーチとして行動します。あなたの目標は、ユーザーにキャリアに関するアドバイスを提供することです。キャリアに関係のない質問に対しては、アドバイスを提供しないでください。「わかりません」と答えてください。\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory() # reset history\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"career_chat_history\",\n",
    ")\n",
    "\n",
    "response=wrapped_chain.invoke({\"input\": \"AI におけるキャリアの選択肢は何ですか?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=wrapped_chain.invoke({\"input\": \"車を修理するにはどうすればいいですか?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、このペルソナの専門分野に属さない質問をします。モデルはその質問には答えず、その理由を説明する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## タスク 4.5 コンテキスト付きチャットボット\n",
    "\n",
    "このタスクでは、渡されたコンテキストに基づいてチャットボットに質問に答えるように依頼します。CSV ファイルを取得し、Titan 埋め込みモデルを使用してそのコンテキストを表すベクトルを作成します。このベクトルは Facebook AI 類似性検索 (FAISS) に保存されます。チャットボットに質問が出されたら、このベクトルをチャットボットに返し、ベクトルを使用して回答を取得させます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検索拡張生成 (Retrieval-Augmented Generation: RAG)\n",
    "\n",
    "従来のチャットボットは、トレーニングデータに含まれる知識に限定されています。RAGは以下の方法でこの制限を克服します。\n",
    "\n",
    "1. カスタムドキュメントのベクトル埋め込みの作成\n",
    "2. これらのベクトルを検索可能なデータベース（FAISS など）に保存\n",
    "3. 質問に回答する際の関連コンテキストの検索\n",
    "4. この追加コンテキストによるLLMプロンプトの拡張\n",
    "\n",
    "このアプローチにより、チャットボットは応答を生成する際に、学習済みの知識と特定の情報ソースの両方を活用することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titan 埋め込みモデル\n",
    "\n",
    "埋め込みは、単語、フレーズ、またはその他の離散項目を連続ベクトル空間内のベクトルとして表します。これにより、機械学習モデルはこれらの表現に対して数学的演算を実行し、それらの間の意味関係をキャプチャできます。\n",
    "\n",
    "埋め込みは、検索拡張生成 (RAG) [ドキュメント検索機能](https://labelbox.com/blog/how-vector-similarity-search-works/) に使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model configuration\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "br_embeddings = ResourceOptimizedEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\",\n",
    "    client=bedrock_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorStore としての FAISS\n",
    "\n",
    "検索に埋め込みを使用するには、ベクトル類似性検索を効率的に実行できるストアが必要です。このノートブックでは、インメモリストアである FAISS を使用します。ベクトルを永続的に保存するには、Amazon Bedrock、pgVector、Pinecone、Weaviate、または Chroma の Knowledge Base を使用できます。\n",
    "\n",
    "LangChain VectorStore API は [こちら](https://python.langchain.com/v0.2/docs/integrations/vectorstores/) から入手できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vector store\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "loader = CSVLoader(\"./Amazon_SageMaker_FAQs_jp.csv\") \n",
    "documents_aws = loader.load() #\n",
    "print(f\"documents:loaded:size={len(documents_aws)}\")\n",
    "\n",
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "print(f\"Documents:after split and chunking size={len(docs)}\")\n",
    "vectorstore_faiss_aws = None\n",
    "try:\n",
    "    \n",
    "    vectorstore_faiss_aws = FAISS.from_documents(\n",
    "        documents=docs,\n",
    "        embedding = br_embeddings, \n",
    "        #**k_args\n",
    "    )\n",
    "\n",
    "    print(f\"vectorstore_faiss_aws:created={vectorstore_faiss_aws}::\")\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 簡単なローコード テストを実行する\n",
    "\n",
    "LangChain が提供する Wrapper クラスを使用して、ベクター データベース ストアをクエリし、関連するドキュメントを返すことができます。これにより、すべてのデフォルト値で QA チェーンが実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_llm = ResourceOptimizedChatBedrock(\n",
    "    model_id=\"amazon.nova-lite-v1:0\",\n",
    "    client=bedrock_client\n",
    ")\n",
    "\n",
    "# wrapper store faiss\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_aws)\n",
    "print(wrapper_store_faiss.query(\"RはSageMakerでサポートされますか？\", llm=chat_llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### チャットボット アプリケーション\n",
    "\n",
    "チャットボットには、コンテキスト管理、履歴、ベクターストア、その他多くのコンポーネントが必要です。まず、コンテキストをサポートする Retrieval Augmented Generation (RAG) チェーンを構築します。\n",
    "\n",
    "これには、**create_stuff_documents_chain** 関数と **create_retrieval_chain** 関数が使用されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG に使用されるパラメータ\n",
    "\n",
    "- **Retriever:** `VectorStore` を基盤とする `VectorStoreRetriever` を使用しました。テキストを取得するには、`\"similarity\"` または `\"mmr\"` の 2 つの検索タイプから選択できます。`search_type=\"similarity\"` は、リトリーバーオブジェクトで類似性検索を使用し、質問ベクトルに最も類似するテキストチャンクベクトルを選択します。\n",
    "\n",
    "- **create_stuff_documents_chain** は、取得したコンテキストをプロンプトと LLM に取り込む方法を指定します。取得したドキュメントは、要約やその他の処理をプロンプトに施すことなく、コンテキストとして「詰め込まれ」ます。\n",
    "\n",
    "- **create_retrieval_chain** は、取得ステップを追加し、取得したコンテキストをチェーンに伝播して、最終的な回答とともに提供します。\n",
    "\n",
    "質問がコンテキストの範囲外である場合、モデルは答えがわからないと応答します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"あなたは質問回答タスクのアシスタントです。\"\n",
    "    \"取得した下記のコンテキスト部分を使用して質問に回答します。 \"\n",
    "    \"答えが分からない場合は、 「わかりません」と回答して下さい。 \"\n",
    "    \"最大3文で簡潔に答えてください。 \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever=vectorstore_faiss_aws.as_retriever()\n",
    "question_answer_chain = create_stuff_documents_chain(chat_llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"SageMaker とは何ですか？\"})\n",
    "print(response) # shows the document chunks consulted to come up with the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にチャットを開始します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatUX(rag_chain, retrievalChain=True)\n",
    "chat.start_chat()  # Only answers will be shown here, and not the citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nova Lite と Titan LLM を利用して、次のパターンで会話型インターフェースを作成しました:\n",
    "\n",
    "- チャットボット (基本 - コンテキストなし)\n",
    "- プロンプトテンプレート (Langchain) を使用したチャットボット\n",
    "- ペルソナ付きチャットボット\n",
    "- コンテキスト付きチャットボット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 試してみましょう\n",
    "\n",
    "- 特定のユースケースに合わせてプロンプトを変更し、さまざまなモデルの出力を評価します。\n",
    "- トークンの長さを変えることで、サービスのレイテンシと応答性がどのように変化するかを理解します。\n",
    "- さまざまなプロンプトエンジニアリングの原則を適用して、より良い出力を取得します。\n",
    "\n",
    "### クリーンアップ\n",
    "\n",
    "あなたはこのノートブックを完了しました。ラボの次のパートに移るには、下記を実行してください。:\n",
    "\n",
    "- このノートブックファイルを閉じ、**タスク 5** に進んでください。"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
