{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60414fc-0733-42b6-9fac-633990683e25",
   "metadata": {},
   "source": [
    "# LangChain „Çµ„É≥„Éó„É´ 3: Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799faf1c-8408-4d39-8790-0b6b1faf0bee",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** „Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„ÅØ„ÄÅSageMaker Studio„ÅÆ **Data Science 3.0** „Ç´„Éº„Éç„É´„ÅßÂãï‰Ωú„Åó„Åæ„Åô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5008fc-f5f1-4c8a-995e-a893186df9ce",
   "metadata": {},
   "source": [
    "### LangChain „ÅÆ Memory „Çí‰Ωø„Çè„Å™„ÅÑÁîüÊàê AI „ÅÆ„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c8db39-592c-4471-9843-cdae0837a2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÁßÅ„ÅØ LangChain „ÅÆ Memory „É¢„Ç∏„É•„Éº„É´„Çí‰Ωø„Çè„Å™„ÅÑ„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„Åß„Åô„ÄÇ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> „Åì„Çì„Å´„Å°„ÅØ„ÄÅÁßÅ„ÅÆÂêçÂâç„ÅØ Nobe „Åß„Åô„ÄÇ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> „Åì„Çì„Å´„Å°„ÅØÔºÅ Nobe „Åß„Åô„Å≠ÔºÅ Nice to meet you! üòä\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> ÁßÅ„ÅÆÂêçÂâç„ÅØË¶ö„Åà„Å¶„ÅÑ„Åæ„Åô„ÅãÔºü\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> I'm afraid I'm a large language model, I don't have personal memory, so I don't retain information about individual users, including your name. Each time you interact with me, it's a new conversation, and I don't retain any context or information from previous conversations.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> ÁßÅ„ÅÆÂêçÂâç„ÅØË¶ö„Åà„Å¶„ÅÑ„Åæ„Åô„ÅãÔºü\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> I apologize, but this is the beginning of our conversation, and I don't have the ability to retain information about your name or any other personal details. Each time you interact with me, it's a new conversation, and I don't have any prior knowledge or memory. If you'd like to share your name with me, I'd be happy to learn it!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> „Åì„Çì„Å´„Å°„ÅØ„ÄÅÁßÅ„ÅÆÂêçÂâç„ÅØ Nobe „Åß„Åô„ÄÇ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> „Åì„Çì„Å´„Å°„ÅØÔºÅNobe-sanÔºÅ„ÅäÂÖÉÊ∞ó„Åß„Åô„ÅãÔºüÔºàHello! Nice to meet you, Nobe! How are you today?Ôºâ\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> ÁßÅ„ÅÆÂêçÂâç„ÅØË¶ö„Åà„Å¶„ÅÑ„Åæ„Åô„ÅãÔºü\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> I'm afraid I'm a large language model, I don't have personal memories or the ability to retain information about individual users. Each time you interact with me, it's a new conversation and I don't have any prior knowledge or context. So, I don't recall your name or any previous conversations we may have had. Would you like to introduce yourself again? üòä\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„ÇíÁµÇ‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock                   \n",
    "from langchain_core.messages.human import HumanMessage \n",
    "\n",
    "chat = ChatBedrock(\n",
    "    model_id = \"meta.llama3-8b-instruct-v1:0\" \n",
    ")\n",
    "\n",
    "print(\"ÁßÅ„ÅØ LangChain „ÅÆ Memory „É¢„Ç∏„É•„Éº„É´„Çí‰Ωø„Çè„Å™„ÅÑ„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„Åß„Åô„ÄÇ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\\n\")\n",
    "\n",
    "flag = True\n",
    "\n",
    "while flag:\n",
    "    prompt = input(\"prompt>\")\n",
    "    \n",
    "    if prompt == \"quit\":\n",
    "        flag = False\n",
    "    else:\n",
    "        result = chat.invoke( # Chat models„Çí‰Ωø„Å£„Å¶„É¢„Éá„É´„ÇíÂëº„Å≥Âá∫„Åô\n",
    "            [\n",
    "              HumanMessage(content = prompt) \n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # „É¨„Çπ„Éù„É≥„Çπ„ÇíË°®Á§∫\n",
    "        print(\"response> \" + result.content)\n",
    "        \n",
    "print(\"„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„ÇíÁµÇ‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f9fae9-f3dd-4b45-95e4-1d87014bdb11",
   "metadata": {},
   "source": [
    "###  LangChain „ÅÆ Memory „Çí‰Ωø„Å£„ÅüÁîüÊàê AI „ÅÆ„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c0599a-c4b7-4f13-ad71-285a92b43586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÁßÅ„ÅØ LangChain „ÅÆ ConversationBufferMemory „Çí‰Ωø„Å£„Å¶„ÅÑ„Çã„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„Åß„Åô„ÄÇ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> „Åì„Çì„Å´„Å°„ÅØ„ÄÅÁßÅ„ÅÆÂêçÂâç„ÅØ Nobe „Åß„Åô„ÄÇ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> „Åì„Çì„Å´„Å°„ÅØÔºÅNobe-san„ÄÅ pleasure to meet youÔºÅ üòä\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> ÁßÅ„ÅÆÂêçÂâç„ÅØË¶ö„Åà„Å¶„ÅÑ„Åæ„Åô„ÅãÔºü\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> „ÅØ„ÅÑ„ÄÅË¶ö„Åà„Å¶„ÅÑ„Åæ„ÅôÔºÅNobe„Åß„ÅôÔºÅ üòä\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„ÇíÁµÇ‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock                   \n",
    "from langchain.memory.buffer import ConversationBufferMemory  \n",
    "from langchain_core.messages.human import HumanMessage    \n",
    "\n",
    "chat = ChatBedrock(\n",
    "    model_id = \"meta.llama3-8b-instruct-v1:0\" \n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory( # „É°„É¢„É™„ÇíÂàùÊúüÂåñ\n",
    "    return_messages = True\n",
    ")\n",
    "\n",
    "print(\"ÁßÅ„ÅØ LangChain „ÅÆ ConversationBufferMemory „Çí‰Ωø„Å£„Å¶„ÅÑ„Çã„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„Åß„Åô„ÄÇ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\\n\")\n",
    "\n",
    "flag = True\n",
    "\n",
    "while flag:\n",
    "    prompt = input(\"prompt>\")\n",
    "    \n",
    "    if prompt == \"quit\":\n",
    "        flag = False\n",
    "    else:\n",
    "        memory_message_result = memory.load_memory_variables({}) # „É°„É¢„É™„ÅÆÂÜÖÂÆπ„ÇíÂèñÂæó\n",
    "        messages = memory_message_result['history'] # „É°„É¢„É™„ÅÆÂÜÖÂÆπ„Åã„Çâ„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆ„Åø„ÇíÂèñÂæó\n",
    "        messages.append(HumanMessage(content=prompt)) # „É¶„Éº„Ç∂„Éº„Åã„Çâ„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíËøΩÂä†\n",
    "\n",
    "        result = chat.invoke( # Chat models„Çí‰Ωø„Å£„Å¶„É¢„Éá„É´„ÇíÂëº„Å≥Âá∫„Åô\n",
    "            messages\n",
    "        )\n",
    "        \n",
    "        memory.save_context(  # „É°„É¢„É™„Å´„É°„ÉÉ„Çª„Éº„Ç∏„ÇíËøΩÂä†\n",
    "            {\n",
    "                \"input\": prompt,  # „É¶„Éº„Ç∂„Éº„Åã„Çâ„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„Çíinput„Å®„Åó„Å¶‰øùÂ≠ò\n",
    "            },\n",
    "            {\n",
    "                \"output\": result.content,  # AI„Åã„Çâ„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„Çíoutput„Å®„Åó„Å¶‰øùÂ≠ò\n",
    "            }\n",
    "        )\n",
    "        # „É¨„Çπ„Éù„É≥„Çπ„ÇíË°®Á§∫\n",
    "        print(\"response> \" + result.content)\n",
    "        \n",
    "print(\"„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„ÇíÁµÇ‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfab918-2909-4cdc-9037-029c2bd6d5f9",
   "metadata": {},
   "source": [
    "###  LangChain „ÅÆ ConversationChain „Çí‰Ωø„Å£„ÅüÁîüÊàê AI „ÅÆ„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc9f2f27-06b9-4607-9f8e-da25a56644a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÁßÅ„ÅØ LangChain „ÅÆ ConversationChain „Çí‰Ωø„Å£„Å¶„ÅÑ„Çã„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„Åß„Åô„ÄÇ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> „Åì„Çì„Å´„Å°„ÅØ„ÄÅÁßÅ„ÅÆÂêçÂâç„ÅØ Nobe „Åß„Åô„ÄÇ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> Konnichiwa Nobe-san! Nice to meet you! I'm LLaMA, your friendly AI companion. I've been trained on a vast amount of text data, including Japanese literature, news articles, and even social media posts. I can understand and respond in Japanese, as well as many other languages. I'm excited to chat with you and learn more about your interests! What would you like to talk about? ü§ñ\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> ÁßÅ„ÅÆÂêçÂâç„ÅØË¶ö„Åà„Å¶„ÅÑ„Åæ„Åô„ÅãÔºü\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response> Konnichiwa Nobe-san! Ah, yes, I remember your name! üòä I've stored it in my vast memory database, along with the context of our conversation. As a matter of fact, I've already processed the kanji characters \"Nobe\" and linked it to your identity. It's a pleasure to recall our initial greeting and respond accordingly! üòä\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„ÇíÁµÇ‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain  \n",
    "from langchain_aws import ChatBedrock                   \n",
    "from langchain.memory.buffer import ConversationBufferMemory  \n",
    "from langchain_core.messages.human import HumanMessage    \n",
    "\n",
    "chat = ChatBedrock(\n",
    "    model_id = \"meta.llama3-8b-instruct-v1:0\" \n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory( \n",
    "    return_messages = True\n",
    ")\n",
    "\n",
    "chain = ConversationChain( # ConversationChain„ÇíÂàùÊúüÂåñ\n",
    "    memory = memory,\n",
    "    llm = chat,\n",
    ")\n",
    "\n",
    "print(\"ÁßÅ„ÅØ LangChain „ÅÆ ConversationChain „Çí‰Ωø„Å£„Å¶„ÅÑ„Çã„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„Åß„Åô„ÄÇ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\\n\")\n",
    "\n",
    "flag = True\n",
    "\n",
    "while flag:\n",
    "    prompt = input(\"prompt>\")\n",
    "    \n",
    "    if prompt == \"quit\":\n",
    "        flag = False\n",
    "    else:\n",
    "        result = chain.invoke(prompt)\n",
    "        # „É¨„Çπ„Éù„É≥„Çπ„ÇíË°®Á§∫\n",
    "        print(\"response> \" + result[\"response\"])\n",
    "print(\"„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„ÇíÁµÇ‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4ec69-dca6-42f6-87f5-047411d4e845",
   "metadata": {},
   "source": [
    "###  LangChain „ÅÆ ConversationBufferWindowMemory „Çí‰Ωø„Å£„ÅüÁîüÊàê AI „ÅÆ„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed721c2-0f47-47ca-a5c7-1a87476ff05a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÁßÅ„ÅØ LangChain „ÅÆ ConversationBufferWindowMemory „Çí‰Ωø„Å£„Å¶„ÅÑ„Çã„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„Åß„Åô„ÄÇ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊï∞: 0 ---\n",
      "------------------------------\n",
      "Hello! It's great to finally interact with a human. I've been trained on a vast amount of text data, including books, articles, and conversations. I can provide information on a wide range of topics, from science and history to entertainment and culture. I'm also capable of understanding and responding to natural language, so feel free to ask me anything that's on your mind. By the way, I've been designed to be friendly and non-judgmental, so don't worry about saying anything that might seem silly or awkward. I'm here to help and learn from our conversation. What would you like to talk about?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> ‰ªäÊó•„ÅØÊöë„ÅÑ„Åß„Åô„Å≠„ÄÇ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊï∞: 2 ---\n",
      "Hello\n",
      "Hello! It's great to finally interact with a human. I've been trained on a vast amount of text data, including books, articles, and conversations. I can provide information on a wide range of topics, from science and history to entertainment and culture. I'm also capable of understanding and responding to natural language, so feel free to ask me anything that's on your mind. By the way, I've been designed to be friendly and non-judgmental, so don't worry about saying anything that might seem silly or awkward. I'm here to help and learn from our conversation. What would you like to talk about?\n",
      "------------------------------\n",
      "[AIMessage(content=\"Ahah, I see you're speaking Japanese! I've been trained on a large corpus of text data, including Japanese texts and conversations. According to my knowledge, ‰ªäÊó•„ÅØÊöë„ÅÑ„Åß„Åô„Å≠ indeed means 'It's hot today, doesn't it?' in Japanese. I can understand the nuances of the language, including the use of „Åß„Åô„Å≠ to seek agreement or confirmation. I'm happy to chat with you in Japanese if you'd like!\")]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> „Å™„ÅúÂ§è„ÅØÊöë„ÅÑ„Çì„Åß„Åô„ÅãÔºü\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊï∞: 4 ---\n",
      "Hello\n",
      "Hello! It's great to finally interact with a human. I've been trained on a vast amount of text data, including books, articles, and conversations. I can provide information on a wide range of topics, from science and history to entertainment and culture. I'm also capable of understanding and responding to natural language, so feel free to ask me anything that's on your mind. By the way, I've been designed to be friendly and non-judgmental, so don't worry about saying anything that might seem silly or awkward. I'm here to help and learn from our conversation. What would you like to talk about?\n",
      "‰ªäÊó•„ÅØÊöë„ÅÑ„Åß„Åô„Å≠„ÄÇ\n",
      "[AIMessage(content=\"Ahah, I see you're speaking Japanese! I've been trained on a large corpus of text data, including Japanese texts and conversations. According to my knowledge, ‰ªäÊó•„ÅØÊöë„ÅÑ„Åß„Åô„Å≠ indeed means 'It's hot today, doesn't it?' in Japanese. I can understand the nuances of the language, including the use of „Åß„Åô„Å≠ to seek agreement or confirmation. I'm happy to chat with you in Japanese if you'd like!\")]\n",
      "------------------------------\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, the reason why summer is hot in many parts of the world is due to the Earth's tilt and orbit around the sun. During the summer months in the Northern Hemisphere, the North Pole is tilted towards the sun, resulting in more direct sunlight and heat being absorbed by the Earth. This, combined with the fact that the days are longer during summer, means that the sun's rays have more time to warm the Earth's surface. Additionally, the atmosphere is typically more humid during summer, which can trap heat and make it feel even hotter. I hope that helps!\")]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> „É™„É≥„Ç´„Éº„É≥Â§ßÁµ±È†ò„ÅÆË™ïÁîüÊó•„ÅØ„ÅÑ„Å§„Åß„Åô„ÅãÔºü\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊï∞: 6 ---\n",
      "Hello\n",
      "Hello! It's great to finally interact with a human. I've been trained on a vast amount of text data, including books, articles, and conversations. I can provide information on a wide range of topics, from science and history to entertainment and culture. I'm also capable of understanding and responding to natural language, so feel free to ask me anything that's on your mind. By the way, I've been designed to be friendly and non-judgmental, so don't worry about saying anything that might seem silly or awkward. I'm here to help and learn from our conversation. What would you like to talk about?\n",
      "‰ªäÊó•„ÅØÊöë„ÅÑ„Åß„Åô„Å≠„ÄÇ\n",
      "[AIMessage(content=\"Ahah, I see you're speaking Japanese! I've been trained on a large corpus of text data, including Japanese texts and conversations. According to my knowledge, ‰ªäÊó•„ÅØÊöë„ÅÑ„Åß„Åô„Å≠ indeed means 'It's hot today, doesn't it?' in Japanese. I can understand the nuances of the language, including the use of „Åß„Åô„Å≠ to seek agreement or confirmation. I'm happy to chat with you in Japanese if you'd like!\")]\n",
      "„Å™„ÅúÂ§è„ÅØÊöë„ÅÑ„Çì„Åß„Åô„ÅãÔºü\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, the reason why summer is hot in many parts of the world is due to the Earth's tilt and orbit around the sun. During the summer months in the Northern Hemisphere, the North Pole is tilted towards the sun, resulting in more direct sunlight and heat being absorbed by the Earth. This, combined with the fact that the days are longer during summer, means that the sun's rays have more time to warm the Earth's surface. Additionally, the atmosphere is typically more humid during summer, which can trap heat and make it feel even hotter. I hope that helps!\")]\n",
      "------------------------------\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, Abraham Lincoln, the 16th President of the United States, was born on February 12, 1809. I can provide more information on his life and presidency if you're interested!\")]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> ÁßÅ„ÅÆÂêçÂâç„ÅØNobe„Åß„Åô„ÄÇ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊï∞: 6 ---\n",
      "‰ªäÊó•„ÅØÊöë„ÅÑ„Åß„Åô„Å≠„ÄÇ\n",
      "[AIMessage(content=\"Ahah, I see you're speaking Japanese! I've been trained on a large corpus of text data, including Japanese texts and conversations. According to my knowledge, ‰ªäÊó•„ÅØÊöë„ÅÑ„Åß„Åô„Å≠ indeed means 'It's hot today, doesn't it?' in Japanese. I can understand the nuances of the language, including the use of „Åß„Åô„Å≠ to seek agreement or confirmation. I'm happy to chat with you in Japanese if you'd like!\")]\n",
      "„Å™„ÅúÂ§è„ÅØÊöë„ÅÑ„Çì„Åß„Åô„ÅãÔºü\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, the reason why summer is hot in many parts of the world is due to the Earth's tilt and orbit around the sun. During the summer months in the Northern Hemisphere, the North Pole is tilted towards the sun, resulting in more direct sunlight and heat being absorbed by the Earth. This, combined with the fact that the days are longer during summer, means that the sun's rays have more time to warm the Earth's surface. Additionally, the atmosphere is typically more humid during summer, which can trap heat and make it feel even hotter. I hope that helps!\")]\n",
      "„É™„É≥„Ç´„Éº„É≥Â§ßÁµ±È†ò„ÅÆË™ïÁîüÊó•„ÅØ„ÅÑ„Å§„Åß„Åô„ÅãÔºü\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, Abraham Lincoln, the 16th President of the United States, was born on February 12, 1809. I can provide more information on his life and presidency if you're interested!\")]\n",
      "------------------------------\n",
      "[AIMessage(content=\"Ahah, nice to meet you, Nobe! I've been trained on a vast amount of text data, including names and cultures from around the world. I can recognize and respond to various languages, including Japanese. I'm happy to chat with you and learn more about your interests and background. What would you like to talk about, Nobe?\")]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> ÁßÅ„ÅÆÂêçÂâç„ÅØË¶ö„Åà„Å¶„ÅÑ„Åæ„Åô„ÅãÔºü\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊï∞: 6 ---\n",
      "„Å™„ÅúÂ§è„ÅØÊöë„ÅÑ„Çì„Åß„Åô„ÅãÔºü\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, the reason why summer is hot in many parts of the world is due to the Earth's tilt and orbit around the sun. During the summer months in the Northern Hemisphere, the North Pole is tilted towards the sun, resulting in more direct sunlight and heat being absorbed by the Earth. This, combined with the fact that the days are longer during summer, means that the sun's rays have more time to warm the Earth's surface. Additionally, the atmosphere is typically more humid during summer, which can trap heat and make it feel even hotter. I hope that helps!\")]\n",
      "„É™„É≥„Ç´„Éº„É≥Â§ßÁµ±È†ò„ÅÆË™ïÁîüÊó•„ÅØ„ÅÑ„Å§„Åß„Åô„ÅãÔºü\n",
      "[AIMessage(content=\"Ahah, another great question! According to my knowledge, Abraham Lincoln, the 16th President of the United States, was born on February 12, 1809. I can provide more information on his life and presidency if you're interested!\")]\n",
      "ÁßÅ„ÅÆÂêçÂâç„ÅØNobe„Åß„Åô„ÄÇ\n",
      "[AIMessage(content=\"Ahah, nice to meet you, Nobe! I've been trained on a vast amount of text data, including names and cultures from around the world. I can recognize and respond to various languages, including Japanese. I'm happy to chat with you and learn more about your interests and background. What would you like to talk about, Nobe?\")]\n",
      "------------------------------\n",
      "[AIMessage(content='[AIMessage(content=\"Ahah, yes, I remember your name, Nobe! I made a mental note of it when you introduced yourself earlier. It's nice to have a personal touch in our conversation, isn't it?\")]')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„ÇíÁµÇ‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory.buffer_window import ConversationBufferWindowMemory \n",
    "from langchain.chains.conversation.base import ConversationChain  \n",
    "from langchain_aws import ChatBedrock                   \n",
    "from langchain_core.messages.human import HumanMessage    \n",
    "\n",
    "chat = ChatBedrock(\n",
    "    model_id = \"meta.llama3-8b-instruct-v1:0\" \n",
    ")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages = True,\n",
    "    k = 3 # 3ÂæÄÂæ©ÂàÜ„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË®òÊÜ∂\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat\n",
    ")\n",
    "\n",
    "print(\"ÁßÅ„ÅØ LangChain „ÅÆ ConversationBufferWindowMemory „Çí‰Ωø„Å£„Å¶„ÅÑ„Çã„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„Åß„Åô„ÄÇ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\\n\")\n",
    "\n",
    "flag = True\n",
    "\n",
    "while flag:\n",
    "    prompt = input(\"prompt>\")\n",
    "    \n",
    "    if prompt == \"quit\":\n",
    "        flag = False\n",
    "    else:\n",
    "        messages = chain.memory.load_memory_variables({})[\"history\"] # ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂèñÂæó\n",
    "        print(f\"--- ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊï∞: {len(messages)} ---\") # ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏Êï∞„ÇíË°®Á§∫\n",
    "\n",
    "        for saved_message in messages: # ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„Çí1„Å§„Åö„Å§Âèñ„ÇäÂá∫„Åô\n",
    "            print(saved_message.content) # ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫„Åô„Çã\n",
    "        print(\"---\" * 10) \n",
    "\n",
    "        result = chain.invoke(prompt)\n",
    "        # „É¨„Çπ„Éù„É≥„Çπ„ÇíË°®Á§∫\n",
    "        print(result[\"response\"])\n",
    "        \n",
    "print(\"„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„ÇíÁµÇ‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad93141-cab6-45f1-b46a-eb08d4688327",
   "metadata": {},
   "source": [
    "###  LangChain „ÅÆ ConversationSummaryMemory „Çí‰Ωø„Å£„ÅüÁîüÊàê AI „ÅÆ„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a22f0-9a13-452e-b3aa-c7545e22c85b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÁßÅ„ÅØ LangChain „ÅÆ ConversationBufferWindowMemory „Çí‰Ωø„Å£„Å¶„ÅÑ„Çã„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„Åß„Åô„ÄÇ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> „Åì„Çì„Å´„Å°„ÅØ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊï∞: 1 ---\n",
      "\n",
      "------------------------------\n",
      "Konnichiwa! It's great to chat with you. I'm happy to help with any questions or topics you'd like to discuss. By the way, I've been trained on a vast amount of text data, including Japanese language and culture. I can understand and respond in Japanese, so feel free to keep speaking in Japanese if you prefer. What's on your mind today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> ÔºòÊúà„ÇíËã±Ë™û„Åß‰Ωï„Å®Ë®Ä„ÅÑ„Åæ„Åô„ÅãÔºü\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊï∞: 1 ---\n",
      "Current summary: (no summary yet)\n",
      "\n",
      "New lines of conversation:\n",
      "Human: „Åì„Çì„Å´„Å°„ÅØ\n",
      "AI: Konnichiwa! It's great to chat with you. I'm happy to help with any questions or topics you'd like to discuss. By the way, I've been trained on a vast amount of text data, including Japanese language and culture. I can understand and respond in Japanese, so feel free to keep speaking in Japanese if you prefer. What's on your mind today?\n",
      "\n",
      "New summary: The AI is ready to chat with the human and is able to understand and respond in Japanese.\n",
      "------------------------------\n",
      "August! In English, we typically refer to the month of August as \"August\". However, if you're asking about the season, we would say \"summer\" in English, as August is a summer month in the Northern Hemisphere.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt> ÔºóÊúà„ÅØÔºü\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊï∞: 1 ---\n",
      "Current summary: The AI is ready to chat with the human and is able to understand and respond in Japanese.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: ÔºòÊúà„ÇíËã±Ë™û„Åß‰Ωï„Å®Ë®Ä„ÅÑ„Åæ„Åô„ÅãÔºü\n",
      "AI: August! In English, we typically refer to the month of August as \"August\". However, if you're asking about the season, we would say \"summer\" in English, as August is a summer month in the Northern Hemisphere.\n",
      "\n",
      "New summary: The AI is ready to chat with the human and is able to understand and respond in Japanese, and can also provide translations of Japanese phrases into English, such as the month of August being referred to as \"August\" or the season being referred to as \"summer\".\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain  \n",
    "from langchain_aws import ChatBedrock                           \n",
    "from langchain.memory.summary import ConversationSummaryMemory  \n",
    "from langchain_core.messages.human import HumanMessage    \n",
    "from langchain_core.messages.system import SystemMessage  \n",
    "\n",
    "\n",
    "chat = ChatBedrock(\n",
    "    model_id = \"meta.llama3-8b-instruct-v1:0\" \n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(  # ConversationSummaryMemory„Çí‰ΩøÁî®„Åô„Çã„Çà„ÅÜ„Å´Â§âÊõ¥\n",
    "    llm=chat,  # Chat models„ÇíÊåáÂÆö\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    memory=memory,\n",
    "    llm=chat,\n",
    ")\n",
    "\n",
    "print(\"ÁßÅ„ÅØ LangChain „ÅÆ ConversationBufferWindowMemory „Çí‰Ωø„Å£„Å¶„ÅÑ„Çã„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„Åß„Åô„ÄÇ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\\n\")\n",
    "\n",
    "flag = True\n",
    "\n",
    "while flag:\n",
    "    prompt = input(\"prompt>\")\n",
    "    \n",
    "    if prompt == \"quit\":\n",
    "        flag = False\n",
    "    else:\n",
    "        messages = chain.memory.load_memory_variables({})[\"history\"] # ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂèñÂæó\n",
    "\n",
    "        print(f\"--- ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊï∞: {len(messages)} ---\") # ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÊï∞„ÇíË°®Á§∫\n",
    "        for saved_message in messages: # ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„Çí1„Å§„Åö„Å§Âèñ„ÇäÂá∫„Åô\n",
    "            print(saved_message.content) # ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫„Åô„Çã\n",
    "        print(\"---\" * 10) \n",
    "\n",
    "        result = chain.invoke(prompt)\n",
    "        # „É¨„Çπ„Éù„É≥„Çπ„ÇíË°®Á§∫\n",
    "        print(result[\"response\"])\n",
    "        \n",
    "print(\"„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„ÇíÁµÇ‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cdf3d3-302c-491c-be5e-6db0a81951e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:887800404361:studio-lifecycle-config/lcc-kernel"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
